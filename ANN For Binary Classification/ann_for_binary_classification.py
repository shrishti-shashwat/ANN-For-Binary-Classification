# -*- coding: utf-8 -*-
"""ANN for Binary Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ZIa3FT07lAYbRrMRA8vCP1g5MBAm9nJ

# Step 1: Installation and Setup
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Step 2: Data preprocessing"""

# Importing the dataset
from google.colab import files
uploaded = files.upload()

print(uploaded)

# Checking the working directory
# using a linux command
! ls

# reading the file
dataset = pd.read_csv('Churn_Modelling (1).csv')

dataset.head()

# Selecting the features of matrix means selecting depending and independing variables
# we have to select all the cloumns for x except rownumber, customerId and surname as
# they are not correlated with the output

# customer is staying with the bank or not (exited) is dependent variable
# dropping all the 4 columns therefore axis = 1

# x is number of independent variables

x = dataset.drop(labels=['RowNumber','CustomerId', 'Surname','Exited'], axis =1 )

# y is number of dependent variable
y = dataset['Exited']

x.head(10)

y.head()

"""Machine Learning models are not capable of processing categorical data thats why we have to change them into numerical data

geography and gender are categorical data
"""

# Encoding the categorical data
from sklearn.preprocessing import LabelEncoder

label_1 = LabelEncoder()
x['Geography'] = label_1.fit_transform(x['Geography'])

x.head(10)

label_2 = LabelEncoder()
x['Gender'] = label_2.fit_transform(x['Gender'])

x.head(10)

"""Due to change of categorical data into numerical data model will take this as  2>1>0 but this is not possible as there is no comparision between categorical data now we have to escape from this dummy variable trap."""

# Generate dummy variables with boolean values
dummy_columns = pd.get_dummies(x['Geography'], drop_first=True)

# Convert the dummy columns to integers (0 and 1)
dummy_columns = dummy_columns.astype(int)

# Drop the original 'Geography' column (if it's still there)
x = x.drop('Geography', axis=1)

# Concatenate the dummy columns back to the original DataFrame
x = pd.concat([x, dummy_columns], axis=1)

x.head(10)

"""The above two newly created columns are representing the 3 values but with no dummy variable trap"""

# Spliting the dataset into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.2, random_state= 0)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

# Scaling dataset
# In the dataset there is large scale difference

# feature scaling

# Convert all column names to strings
# Convert all column names to strings
from sklearn.preprocessing import StandardScaler

# Initialize the scaler
sc = StandardScaler()

# Apply the scaler to your NumPy arrays
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""# Step 3: Building the Model"""

# creating an object
# Initializing the ANN
# to make the model we are using the sequential class because we are making a fully connected network

model = tf.keras.models.Sequential()

# adding input layer and first hidden layer

# we can take any number of inputs but we are taking inputs as 6
# because we have input dimension 11 and output dimension 1 so we take the average of them which is 6

# 1) units = 6 (number of nodes = 6)
# 2) activation function = ReLU (Rectified Linear Unit)
# 3) input dimension = 11

model.add(tf.keras.layers.Dense(units=6, activation='relu',input_dim = 11))

# Adding second hidden layer

model.add(tf.keras.layers.Dense(units=6, activation='relu'))

# Output Layer
# units as 1 because we have only 2 outputs 0 or 1
# because of binary output activation function as sigmoid

model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Compiling the model

model.compile(optimizer='adam', loss ='binary_crossentropy', metrics=['accuracy'])

model.summary()

"""# Step 4: Training the model"""

model.fit(x_train,y_train.to_numpy(), batch_size=10, epochs= 20)

"""y_train has only 1 output as it is a vector but x_train is a numpy array so we have to change y_train to a numpy array by using to_numpy()

epochs 20 means we are passing the data 20 times in the model and we are passing this data in a batch of 10

# Step 5: Model evaluation and prediction
"""

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test.to_numpy())

print(f'Test Accuracy: {test_acc}')

y_pred = (model.predict(x_test) > 0.5).astype("int32")

print(y_pred)

print(y_test)

y_test = y_test.to_numpy()

print(y_pred[0]), print(y_test[0])

print(y_pred[0]), print(y_test[0])

# Confusin matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Calculating accuracy on basis of confusion matrix

acc_cm = accuracy_score(y_test, y_pred)
print(acc_cm)

